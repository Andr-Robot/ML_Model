{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description\n",
    "这里使用TensorFlow 1.5实现用于CTR预测的FM，数据集选用的是kaggle上的criteo数据集。下载链接：[http://labs.criteo.com/2014/02/kaggle-display-advertising-challenge-dataset/](http://labs.criteo.com/2014/02/kaggle-display-advertising-challenge-dataset/) \n",
    "\n",
    "数据集介绍：   \n",
    "这是criteo-Display Advertising Challenge比赛的部分数据集， 里面有train.csv和test.csv两个文件：\n",
    "* train.csv： 训练集由Criteo 7天内的部分流量组成。每一行对应一个由Criteo提供的显示广告。为了减少数据集的大小，正(点击)和负(未点击)的例子都以不同的比例进行了抽样。示例是按时间顺序排列的\n",
    "* test.csv: 测试集的计算方法与训练集相同，只是针对训练期之后一天的事件\n",
    "\n",
    "字段说明：\n",
    "* Label： 目标变量， 0表示未点击， 1表示点击\n",
    "* l1-l13: 13列的数值特征， 大部分是计数特征\n",
    "* C1-C26: 26列分类特征， 为了达到匿名的目的， 这些特征的值离散成了32位的数据表示\n",
    "\n",
    "这个比赛的任务就是：开发预测广告点击率(CTR)的模型。给定一个用户和他正在访问的页面，预测他点击给定广告的概率是多少？比赛的地址链接：[https://www.kaggle.com/c/criteo-display-ad-challenge/overview](https://www.kaggle.com/c/criteo-display-ad-challenge/overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FM\n",
    "FM模型方程：   \n",
    "$$y=w_0+\\sum_{i=1}^nw_ix_i+\\sum_{i=1}^{n- 1}\\sum_{j=i+1}^{n}\\langle v_i,v_j \\rangle x_ix_j$$\n",
    "其中，$v_i$是第$i$维特征的隐向量，$\\langle \\cdot, \\cdot \\rangle$代表向量点积，$\\langle v_i, v_j \\rangle=\\sum_{f=1}^k v_{if}v_{jf}$。隐向量的长度为$k(k\\ll n)$，包含 $k$ 个描述特征的因子。   \n",
    "\n",
    "从FM的公式可以看出时间复杂度为$O(kn^2)$，因为所有的交叉特征都需要计算。但是通过二次化简可以将时间复杂度优化到$O(kn)$，化简结果如下：   \n",
    "$$y=w_0+\\sum_{i=1}^nw_ix_i+\\frac{1}{2}\\sum_{f=1}^k\\Bigg[\\bigg(\\sum_{i=1}^n v_{i,f}x_i\\bigg)^2-\\sum_{i=1}^n (v_{i,f})^2x_i^2\\Bigg]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dense特征空值用0填充，并取对数， sparse特征空值用'-1'填充\n",
    "def process_feat(data, dense_feats, sparse_feats):\n",
    "    df = data.copy()\n",
    "    # dense\n",
    "    df[dense_feats] = df[dense_feats].fillna(0.0)\n",
    "    for f in tqdm(dense_feats):\n",
    "        df[f] = df[f].apply(lambda x: np.log(1 + x) if x > -1 else -1)\n",
    "    # sparse\n",
    "    df[sparse_feats] = df[sparse_feats].fillna('-1')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:08<00:00,  1.47it/s]\n"
     ]
    }
   ],
   "source": [
    "# 数据加载\n",
    "file = '../dataset/criteo_sampled_data.csv'\n",
    "data = pd.read_csv(file, sep=',')\n",
    "# dense 特征开头是I, sparse特征开头是C， label是标签\n",
    "cols = data.columns.values\n",
    "dense_feats = [f for f in cols if f[0] == 'I']\n",
    "sparse_feats = [f for f in cols if f[0] == 'C']\n",
    "ignore_feats = ['label']\n",
    "# 数据预处理\n",
    "data_new = process_feat(data, dense_feats, sparse_feats)\n",
    "# 切分训练集和验证集\n",
    "train, test = train_test_split(data_new, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 遍历数据获取对应feature_dict，total_feature\n",
    "def get_feature_dict(data, ignore_feats, dense_feats):\n",
    "    feature_dict = {}\n",
    "    total_feature = 0\n",
    "    for col in tqdm(data.columns):\n",
    "        if col in ignore_feats:\n",
    "            continue\n",
    "        elif col in dense_feats:\n",
    "            feature_dict[col] = total_feature\n",
    "            total_feature += 1\n",
    "        else:\n",
    "            unique_val = data[col].unique()\n",
    "            feature_dict[col] = dict(\n",
    "                zip(unique_val,\n",
    "                    range(total_feature,\n",
    "                        len(unique_val) + total_feature)))\n",
    "            total_feature += len(unique_val)\n",
    "    return feature_dict, total_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_tran(data, feature_dict, ignore_feats, dense_feats):\n",
    "    labels = data['label']\n",
    "    # 这里存储的是每个值对应在feature_dict的idx，将每一条数据转换为对应的特征索引\n",
    "    feature_index = data.copy()\n",
    "    # 这里存储的是每个值，将每一条数据转换为对应的特征值\n",
    "    feature_value = data.copy()\n",
    "    for col in tqdm(feature_index.columns):\n",
    "        if col in ignore_feats:\n",
    "            feature_index.drop(col, axis=1, inplace=True)\n",
    "            feature_value.drop(col, axis=1, inplace=True)\n",
    "        elif col in dense_feats:\n",
    "            feature_index[col] = feature_dict[col]\n",
    "        else:\n",
    "            feature_index[col] = feature_index[col].map(feature_dict[col])\n",
    "            feature_value[col] = 1\n",
    "    return feature_index, feature_value, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:01<00:00, 26.56it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_feature: 885697\n",
      "feature_dict size: 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:07<00:00,  5.00it/s]\n",
      "100%|██████████| 40/40 [00:03<00:00, 12.92it/s]\n"
     ]
    }
   ],
   "source": [
    "feature_dict, total_feature = get_feature_dict(data_new, ignore_feats, dense_feats)\n",
    "print('total_feature:', total_feature)\n",
    "print('feature_dict size:', len(feature_dict))\n",
    "# 产出用于训练的数据\n",
    "train_feature_index, train_feature_value, train_labels = data_tran(\n",
    "    train, feature_dict, ignore_feats, dense_feats)\n",
    "test_feature_index, test_feature_value, test_labels = data_tran(\n",
    "    test, feature_dict, ignore_feats, dense_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_params = {\n",
    "    'embedding_size': 8,\n",
    "    'batch_size': 4000,\n",
    "    'learning_rate': 0.001,\n",
    "    'epoch': 20,\n",
    "    'optimizer': 'adagrad'\n",
    "}\n",
    "fm_params['feature_size'] = total_feature\n",
    "fm_params['field_size'] = len(train_feature_index.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 开始构建模型\n",
    "tf.reset_default_graph()  # 重置网络结构\n",
    "# 定义模型输入\n",
    "# 训练模型的输入有三个，分别是刚才转换得到的特征索引和特征值，以及label：\n",
    "feat_index = tf.placeholder(tf.int32,\n",
    "                            shape=[None, fm_params['field_size']],\n",
    "                            name='feat_index')\n",
    "feat_value = tf.placeholder(tf.float32,\n",
    "                            shape=[None, fm_params['field_size']],\n",
    "                            name='feat_value')\n",
    "labels = tf.placeholder(tf.int32, shape=[None], name='labels')\n",
    "# tf fm weights\n",
    "weights = dict()\n",
    "weights_initializer = tf.glorot_normal_initializer()\n",
    "bias_initializer = tf.constant_initializer(0.0)\n",
    "weights[\"feature_embeddings\"] = tf.get_variable(\n",
    "    name='weights',\n",
    "    dtype=tf.float32,\n",
    "    initializer=weights_initializer,\n",
    "    regularizer=tf.contrib.layers.l2_regularizer(scale=1e-5),\n",
    "    shape=[fm_params['feature_size'], fm_params['embedding_size']])\n",
    "weights[\"weights_first_order\"] = tf.get_variable(\n",
    "    name='vectors',\n",
    "    dtype=tf.float32,\n",
    "    initializer=weights_initializer,\n",
    "    regularizer=tf.contrib.layers.l2_regularizer(1e-5),\n",
    "    shape=[fm_params['field_size'], 1])\n",
    "weights[\"fm_bias\"] = tf.get_variable(name='bias',\n",
    "                                     dtype=tf.float32,\n",
    "                                     initializer=bias_initializer,\n",
    "                                     shape=[1])\n",
    "embeddings = tf.nn.embedding_lookup(weights[\"feature_embeddings\"], feat_index) # shape=(?, 39, 8)\n",
    "bias = weights['fm_bias']\n",
    "#build function\n",
    "##first order\n",
    "first_order = tf.matmul(feat_value,\n",
    "                        weights[\"weights_first_order\"])  # shape=(?, 1)\n",
    "\n",
    "##second order\n",
    "### feature * embeddings\n",
    "reshaped_feat_value = tf.reshape(feat_value,\n",
    "                                 shape=[-1, fm_params['field_size'], 1])\n",
    "# multiply这个函数实现的是元素级别的相乘，也就是两个相乘的数元素各自相乘，而不是矩阵乘法\n",
    "f_e_m = tf.multiply(reshaped_feat_value, embeddings)\n",
    "###  square(sum(feature * embedding))\n",
    "f_e_m_sum = tf.reduce_sum(f_e_m, 1)\n",
    "f_e_m_sum_square = tf.square(f_e_m_sum)\n",
    "###  sum(square(feature * embedding))\n",
    "f_e_m_square = tf.square(f_e_m)\n",
    "f_e_m_square_sum = tf.reduce_sum(f_e_m_square, 1)\n",
    "second_order = f_e_m_sum_square - f_e_m_square_sum\n",
    "second_order = tf.reduce_sum(second_order, 1, keepdims=True)\n",
    "\n",
    "##final objective function\n",
    "logits = second_order + first_order + bias\n",
    "predicts = tf.sigmoid(logits)\n",
    "\n",
    "##loss function\n",
    "new_labels = tf.cast(tf.reshape(labels, shape=[-1, 1]), dtype=tf.float32)\n",
    "sigmoid_loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits,\n",
    "                                                       labels=new_labels)\n",
    "sigmoid_loss = tf.reduce_mean(sigmoid_loss)\n",
    "l2_loss = tf.losses.get_regularization_loss()\n",
    "loss = sigmoid_loss + l2_loss\n",
    "\n",
    "# train op\n",
    "if fm_params['optimizer'] == 'adagrad':\n",
    "    optimizer = tf.train.AdagradOptimizer(\n",
    "        learning_rate=fm_params['learning_rate'],\n",
    "        initial_accumulator_value=1e-8)\n",
    "elif fm_params['optimizer'] == 'adam':\n",
    "    optimizer = tf.train.AdamOptimizer(\n",
    "        learning_rate=fm_params['learning_rate'])\n",
    "else:\n",
    "    raise Exception('unknown optimizer', fm_params['optimizer'])\n",
    "train_op = optimizer.minimize(loss)\n",
    "\n",
    "# accuracy\n",
    "one_tensor = tf.ones_like(predicts)\n",
    "neg_predicts = tf.subtract(one_tensor, predicts)\n",
    "prediction = tf.concat([neg_predicts, predicts], axis=1)\n",
    "# new_labels = tf.cast(tf.reshape(labels, shape=[-1]), dtype=tf.int32)\n",
    "# 如果labels的输入shape是[None, 1]则需要转成[None,]，这样才能供in_top_k使用\n",
    "correct_prediction = tf.nn.in_top_k(prediction, labels, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 0.756251098 acc= 0.481272918 valid_loss= 0.613053322 valid_acc= 0.701250017\n",
      "Epoch: 0002 cost= 0.570809954 acc= 0.741264583 valid_loss= 0.547638834 valid_acc= 0.752933323\n",
      "Epoch: 0003 cost= 0.532172237 acc= 0.761231250 valid_loss= 0.526202083 valid_acc= 0.761108339\n",
      "Epoch: 0004 cost= 0.514010576 acc= 0.768241668 valid_loss= 0.513945758 valid_acc= 0.766366661\n",
      "Epoch: 0005 cost= 0.501640464 acc= 0.773610419 valid_loss= 0.505695403 valid_acc= 0.769391656\n",
      "Epoch: 0006 cost= 0.492207383 acc= 0.778025000 valid_loss= 0.499745578 valid_acc= 0.772000015\n",
      "Epoch: 0007 cost= 0.484651676 acc= 0.781760417 valid_loss= 0.495292366 valid_acc= 0.773783326\n",
      "Epoch: 0008 cost= 0.478405937 acc= 0.784772916 valid_loss= 0.491876841 valid_acc= 0.774824977\n",
      "Epoch: 0009 cost= 0.473113014 acc= 0.787108332 valid_loss= 0.489202827 valid_acc= 0.775550008\n",
      "Epoch: 0010 cost= 0.468529478 acc= 0.789437503 valid_loss= 0.487067133 valid_acc= 0.775908351\n",
      "Epoch: 0011 cost= 0.464482887 acc= 0.791356252 valid_loss= 0.485329509 valid_acc= 0.776158333\n",
      "Epoch: 0012 cost= 0.460849184 acc= 0.793164584 valid_loss= 0.483890980 valid_acc= 0.776525021\n",
      "Epoch: 0013 cost= 0.457538064 acc= 0.795045833 valid_loss= 0.482676923 valid_acc= 0.776783347\n",
      "Epoch: 0014 cost= 0.454483372 acc= 0.796770836 valid_loss= 0.481638312 valid_acc= 0.777125001\n",
      "Epoch: 0015 cost= 0.451636010 acc= 0.798297917 valid_loss= 0.480738252 valid_acc= 0.777433336\n",
      "Epoch: 0016 cost= 0.448959015 acc= 0.799799999 valid_loss= 0.479948252 valid_acc= 0.777649999\n",
      "Epoch: 0017 cost= 0.446424312 acc= 0.801260417 valid_loss= 0.479250669 valid_acc= 0.777516663\n",
      "Epoch: 0018 cost= 0.444010114 acc= 0.802677084 valid_loss= 0.478627980 valid_acc= 0.777758360\n",
      "Epoch: 0019 cost= 0.441699454 acc= 0.803983336 valid_loss= 0.478068829 valid_acc= 0.778208315\n",
      "Epoch: 0020 cost= 0.439478842 acc= 0.805247916 valid_loss= 0.477568030 valid_acc= 0.778408349\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(fm_params['epoch']):\n",
    "        avg_cost = 0.\n",
    "        avg_acc = 0.\n",
    "        total_batch = int(train.shape[0] / fm_params['batch_size'])\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            start_idx = i * fm_params['batch_size']\n",
    "            end_idx = (i + 1) * fm_params['batch_size']\n",
    "            batch_index = train_feature_index[start_idx:end_idx]\n",
    "            batch_value = train_feature_value[start_idx:end_idx]\n",
    "            batch_labels = train_labels[start_idx:end_idx]\n",
    "            # Fit training using batch data\n",
    "            _, c, acc = sess.run(\n",
    "                [train_op, loss, accuracy],\n",
    "                feed_dict={\n",
    "                    feat_index: batch_index,\n",
    "                    feat_value: batch_value,\n",
    "                    labels: batch_labels\n",
    "                })\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "            avg_acc += acc / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if (epoch + 1) % 1 == 0:\n",
    "            vloss, pred1, pred2, cprediction, vacc = sess.run(\n",
    "                [loss, predicts, prediction, correct_prediction, accuracy],\n",
    "                feed_dict={\n",
    "                    feat_index: test_feature_index,\n",
    "                    feat_value: test_feature_value,\n",
    "                    labels: test_labels\n",
    "                })\n",
    "            print('Epoch:', '%04d' % (epoch + 1), 'cost=',\n",
    "                  '{:.9f}'.format(avg_cost), 'acc=', '{:.9f}'.format(avg_acc),\n",
    "                  'valid_loss=', '{:.9f}'.format(vloss), 'valid_acc=',\n",
    "                  '{:.9f}'.format(vacc))\n",
    "\n",
    "    print('Optimization Finished!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b4827ba0a60287222e0734208e739e577ad99c99756b6b76b9edc625c5645729"
  },
  "kernelspec": {
   "display_name": "Python 3.6.12 ('tfenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
