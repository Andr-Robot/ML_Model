{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description\n",
    "这里使用TensorFlow 1.5实现用于CTR预测的DeepFM，数据集选用的是kaggle上的criteo数据集。数据集的详细介绍可以参看[FM代码实现](https://github.com/Andr-Robot/ML_Model/blob/main/FM.ipynb)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# DeepFM\n",
    "DeepFM模型包含FM和DNN两部分，FM模型可以抽取low-order特征，DNN可以抽取high-order特征。相比于Wide & Deep模型无需人工特征工程。由于输入仅为原始特征，而且FM和DNN共享输入向量特征，DeepFM模型训练速度很快。    \n",
    "\n",
    "<div align=center><img src=\"https://raw.githubusercontent.com/Andr-Robot/iMarkdownPhotos/master/Res/ml/deepfm_architecture.png\" width=\"50%;\" style=\"float:center\"/></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dense特征空值用0填充，并取对数， sparse特征空值用'-1'填充\n",
    "def process_feat(data, dense_feats, sparse_feats):\n",
    "    df = data.copy()\n",
    "    # dense\n",
    "    df[dense_feats] = df[dense_feats].fillna(0.0)\n",
    "    for f in tqdm(dense_feats):\n",
    "        df[f] = df[f].apply(lambda x: np.log(1 + x) if x > -1 else -1)\n",
    "    # sparse\n",
    "    df[sparse_feats] = df[sparse_feats].fillna('-1')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:09<00:00,  1.32it/s]\n"
     ]
    }
   ],
   "source": [
    "# 数据加载\n",
    "file = './dataset/criteo_sampled_data.csv'\n",
    "data = pd.read_csv(file, sep=',')\n",
    "# dense 特征开头是I, sparse特征开头是C， label是标签\n",
    "cols = data.columns.values\n",
    "dense_feats = [f for f in cols if f[0] == 'I']\n",
    "sparse_feats = [f for f in cols if f[0] == 'C']\n",
    "ignore_feats = ['label']\n",
    "# 数据预处理\n",
    "data_new = process_feat(data, dense_feats, sparse_feats)\n",
    "# 切分训练集和验证集\n",
    "train, test = train_test_split(data_new, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 遍历数据获取对应feature_dict，total_feature\n",
    "def get_feature_dict(data, ignore_feats, dense_feats):\n",
    "    feature_dict = {}\n",
    "    total_feature = 0\n",
    "    for col in tqdm(data.columns):\n",
    "        if col in ignore_feats:\n",
    "            continue\n",
    "        elif col in dense_feats:\n",
    "            feature_dict[col] = total_feature\n",
    "            total_feature += 1\n",
    "        else:\n",
    "            unique_val = data[col].unique()\n",
    "            feature_dict[col] = dict(\n",
    "                zip(unique_val,\n",
    "                    range(total_feature,\n",
    "                          len(unique_val) + total_feature)))\n",
    "            total_feature += len(unique_val)\n",
    "    return feature_dict, total_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 遍历数据获取对应feature_dict，total_feature\n",
    "def get_feature_dict(data, ignore_feats, dense_feats):\n",
    "    feature_dict = {}\n",
    "    total_feature = 0\n",
    "    for col in tqdm(data.columns):\n",
    "        if col in ignore_feats:\n",
    "            continue\n",
    "        elif col in dense_feats:\n",
    "            feature_dict[col] = total_feature\n",
    "            total_feature += 1\n",
    "        else:\n",
    "            unique_val = data[col].unique()\n",
    "            feature_dict[col] = dict(\n",
    "                zip(unique_val,\n",
    "                    range(total_feature,\n",
    "                          len(unique_val) + total_feature)))\n",
    "            total_feature += len(unique_val)\n",
    "    return feature_dict, total_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对原始特征进行转换，便于后续训练\n",
    "def data_tran(data, feature_dict, ignore_feats, dense_feats):\n",
    "    labels = data['label']\n",
    "    # 这里存储的是每个值对应在feature_dict的idx，将每一条数据转换为对应的特征索引\n",
    "    feature_index = data.copy()\n",
    "    # 这里存储的是每个值，将每一条数据转换为对应的特征值\n",
    "    feature_value = data.copy()\n",
    "    for col in tqdm(feature_index.columns):\n",
    "        if col in ignore_feats:\n",
    "            feature_index.drop(col, axis=1, inplace=True)\n",
    "            feature_value.drop(col, axis=1, inplace=True)\n",
    "        elif col in dense_feats:\n",
    "            feature_index[col] = feature_dict[col]\n",
    "        else:\n",
    "            feature_index[col] = feature_index[col].map(feature_dict[col])\n",
    "            feature_value[col] = 1\n",
    "    return feature_index, feature_value, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:01<00:00, 35.97it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_feature: 885697\n",
      "feature_dict size: 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:08<00:00,  4.48it/s]\n",
      "100%|██████████| 40/40 [00:03<00:00, 11.41it/s]\n"
     ]
    }
   ],
   "source": [
    "feature_dict, total_feature = get_feature_dict(data_new, ignore_feats,\n",
    "                                               dense_feats)\n",
    "print('total_feature:', total_feature)\n",
    "print('feature_dict size:', len(feature_dict))\n",
    "# 产出用于训练的数据\n",
    "train_feature_index, train_feature_value, train_labels = data_tran(\n",
    "    train, feature_dict, ignore_feats, dense_feats)\n",
    "test_feature_index, test_feature_value, test_labels = data_tran(\n",
    "    test, feature_dict, ignore_feats, dense_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepfm_params = {\n",
    "    'embedding_size': 8,\n",
    "    'batch_size': 2000,\n",
    "    'learning_rate': 0.00001,\n",
    "    'epoch': 10,\n",
    "    'optimizer': 'adam',\n",
    "    'dnn_dropout': 0.5,\n",
    "    'hidden_units': [256, 128, 64]\n",
    "}\n",
    "deepfm_params['feature_size'] = total_feature\n",
    "deepfm_params['field_size'] = len(train_feature_index.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 开始构建模型\n",
    "tf.reset_default_graph()  # 重置网络结构\n",
    "# 定义模型输入\n",
    "# 训练模型的输入有三个，分别是刚才转换得到的特征索引和特征值，以及label：\n",
    "feat_index = tf.placeholder(tf.int32,\n",
    "                            shape=[None, deepfm_params['field_size']],\n",
    "                            name='feat_index')\n",
    "feat_value = tf.placeholder(tf.float32,\n",
    "                            shape=[None, deepfm_params['field_size']],\n",
    "                            name='feat_value')\n",
    "labels = tf.placeholder(tf.int32, shape=[None], name='labels')\n",
    "training = tf.placeholder_with_default(False, shape=[], name='training')\n",
    "\n",
    "'''FM part'''\n",
    "# tf fm weights\n",
    "weights = dict()\n",
    "weights_initializer = tf.glorot_normal_initializer()\n",
    "bias_initializer = tf.constant_initializer(0.0)\n",
    "weights[\"feature_embeddings\"] = tf.get_variable(\n",
    "    name='weights',\n",
    "    dtype=tf.float32,\n",
    "    initializer=weights_initializer,\n",
    "    regularizer=tf.contrib.layers.l2_regularizer(scale=1e-5),\n",
    "    shape=[deepfm_params['feature_size'], deepfm_params['embedding_size']])\n",
    "weights[\"weights_first_order\"] = tf.get_variable(\n",
    "    name='vectors',\n",
    "    dtype=tf.float32,\n",
    "    initializer=weights_initializer,\n",
    "    regularizer=tf.contrib.layers.l2_regularizer(1e-5),\n",
    "    shape=[deepfm_params['field_size'], 1])\n",
    "weights[\"fm_bias\"] = tf.get_variable(name='bias',\n",
    "                                     dtype=tf.float32,\n",
    "                                     initializer=bias_initializer,\n",
    "                                     shape=[1])\n",
    "embeddings = tf.nn.embedding_lookup(weights[\"feature_embeddings\"],\n",
    "                                    feat_index)  # shape=(?, 39, 8)\n",
    "bias = weights['fm_bias']\n",
    "#build function\n",
    "##first order\n",
    "first_order = tf.matmul(feat_value,\n",
    "                        weights[\"weights_first_order\"])  # shape=(?, 1)\n",
    "##second order\n",
    "### feature * embeddings\n",
    "reshaped_feat_value = tf.reshape(feat_value,\n",
    "                                 shape=[-1, deepfm_params['field_size'], 1])\n",
    "f_e_m = tf.multiply(\n",
    "    reshaped_feat_value,\n",
    "    embeddings)  # multiply这个函数实现的是元素级别的相乘，也就是两个相乘的数元素各自相乘，而不是矩阵乘法\n",
    "###  square(sum(feature * embedding))\n",
    "f_e_m_sum = tf.reduce_sum(f_e_m, 1)\n",
    "f_e_m_sum_square = tf.square(f_e_m_sum)\n",
    "###  sum(square(feature * embedding))\n",
    "f_e_m_square = tf.square(f_e_m)\n",
    "f_e_m_square_sum = tf.reduce_sum(f_e_m_square, 1)\n",
    "second_order = f_e_m_sum_square - f_e_m_square_sum\n",
    "second_order = 0.5 * tf.reduce_sum(second_order, 1, keepdims=True)\n",
    "##FM part objective function\n",
    "fm_logits = second_order + first_order + bias\n",
    "\n",
    "'''DNN part'''\n",
    "# 这里相当于是将(?, 39, 8)中的第三维展开变成(?, 39 * 8)\n",
    "x = tf.reshape(embeddings, shape=[-1, deepfm_params['field_size'] * deepfm_params['embedding_size']])\n",
    "for i, hidden_unit in enumerate(deepfm_params['hidden_units']):\n",
    "    x = tf.layers.dense(inputs=x, units=hidden_unit, activation=tf.nn.relu, name='hidden_%d' % i)\n",
    "x = tf.layers.dropout(inputs=x, rate=deepfm_params['dnn_dropout'], training=training)\n",
    "##Deep part objective function\n",
    "deep_logits = tf.layers.dense(inputs=x, units=1, activation=None)\n",
    "\n",
    "'''DeepFM output'''\n",
    "logits = tf.add(fm_logits, deep_logits)\n",
    "predicts = tf.sigmoid(logits)\n",
    "\n",
    "##loss function\n",
    "new_labels = tf.cast(tf.reshape(labels, shape=[-1, 1]), dtype=tf.float32)\n",
    "sigmoid_loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits,\n",
    "                                                       labels=new_labels)\n",
    "sigmoid_loss = tf.reduce_mean(sigmoid_loss)\n",
    "l2_loss = tf.losses.get_regularization_loss()\n",
    "loss = sigmoid_loss + l2_loss\n",
    "\n",
    "# train op\n",
    "if deepfm_params['optimizer'] == 'adagrad':\n",
    "    optimizer = tf.train.AdagradOptimizer(\n",
    "        learning_rate=deepfm_params['learning_rate'],\n",
    "        initial_accumulator_value=1e-8)\n",
    "elif deepfm_params['optimizer'] == 'adam':\n",
    "    optimizer = tf.train.AdamOptimizer(\n",
    "        learning_rate=deepfm_params['learning_rate'])\n",
    "else:\n",
    "    raise Exception('unknown optimizer', deepfm_params['optimizer'])\n",
    "train_op = optimizer.minimize(loss)\n",
    "\n",
    "# accuracy\n",
    "one_tensor = tf.ones_like(predicts)\n",
    "neg_predicts = tf.subtract(one_tensor, predicts)\n",
    "prediction = tf.concat([neg_predicts, predicts], axis=1)\n",
    "# new_labels = tf.cast(tf.reshape(labels, shape=[-1]), dtype=tf.int32)\n",
    "# 如果labels的输入shape是[None, 1]则需要转成[None,]，这样才能供in_top_k使用\n",
    "correct_prediction = tf.nn.in_top_k(prediction, labels, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 1.873662378 acc= 0.256718750 valid_loss= 1.780948043 valid_acc= 0.259458333\n",
      "Epoch: 0002 cost= 1.642391616 acc= 0.259768750 valid_loss= 1.449141264 valid_acc= 0.268725008\n",
      "Epoch: 0003 cost= 1.194329687 acc= 0.304004167 valid_loss= 0.915479779 valid_acc= 0.370358348\n",
      "Epoch: 0004 cost= 0.771444572 acc= 0.515295834 valid_loss= 0.642312646 valid_acc= 0.652316689\n",
      "Epoch: 0005 cost= 0.658318720 acc= 0.656402083 valid_loss= 0.607389033 valid_acc= 0.720274985\n",
      "Epoch: 0006 cost= 0.641649251 acc= 0.683527084 valid_loss= 0.598297477 valid_acc= 0.727641642\n",
      "Epoch: 0007 cost= 0.629298873 acc= 0.692437499 valid_loss= 0.589931548 valid_acc= 0.729099989\n",
      "Epoch: 0008 cost= 0.616572655 acc= 0.699260417 valid_loss= 0.580828309 valid_acc= 0.731683314\n",
      "Epoch: 0009 cost= 0.602857007 acc= 0.705931248 valid_loss= 0.571716487 valid_acc= 0.734591663\n",
      "Epoch: 0010 cost= 0.587562498 acc= 0.714556253 valid_loss= 0.563073218 valid_acc= 0.737166643\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(deepfm_params['epoch']):\n",
    "        avg_cost = 0.\n",
    "        avg_acc = 0.\n",
    "        total_batch = int(train.shape[0] / deepfm_params['batch_size'])\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            start_idx = i * deepfm_params['batch_size']\n",
    "            end_idx = (i + 1) * deepfm_params['batch_size']\n",
    "            batch_index = train_feature_index[start_idx:end_idx]\n",
    "            batch_value = train_feature_value[start_idx:end_idx]\n",
    "            batch_labels = train_labels[start_idx:end_idx]\n",
    "            # Fit training using batch data\n",
    "            _, c, acc = sess.run(\n",
    "                [train_op, loss, accuracy],\n",
    "                feed_dict={\n",
    "                    feat_index: batch_index,\n",
    "                    feat_value: batch_value,\n",
    "                    labels: batch_labels,\n",
    "                    training: True\n",
    "                })\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "            avg_acc += acc / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if (epoch + 1) % 1 == 0:\n",
    "            vloss, pred1, pred2, cprediction, vacc = sess.run(\n",
    "                [loss, predicts, prediction, correct_prediction, accuracy],\n",
    "                feed_dict={\n",
    "                    feat_index: test_feature_index,\n",
    "                    feat_value: test_feature_value,\n",
    "                    labels: test_labels\n",
    "                })\n",
    "            print('Epoch:', '%04d' % (epoch + 1), 'cost=',\n",
    "                  '{:.9f}'.format(avg_cost), 'acc=', '{:.9f}'.format(avg_acc),\n",
    "                  'valid_loss=', '{:.9f}'.format(vloss), 'valid_acc=',\n",
    "                  '{:.9f}'.format(vacc))\n",
    "\n",
    "    print('Optimization Finished!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b4827ba0a60287222e0734208e739e577ad99c99756b6b76b9edc625c5645729"
  },
  "kernelspec": {
   "display_name": "Python 3.6.12 ('tfenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
