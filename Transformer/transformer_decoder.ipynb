{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 描述    \n",
    "基于pytorch实现Transformer的decoder模块（非传统版本，是CausalLM decoder）   \n",
    "\n",
    "区别：\n",
    "- 传统Decoder: 包含自注意力层+编码器-解码器注意力层+前馈网络，依赖编码器的输出（如机器翻译中的源语言编码），典型模型Transformer、BART、T5\n",
    "- CausalLM decoder: 仅包含自注意力层+前馈网络，仅依赖自身生成的序列（无编码器输入），典型模型GPT系列、LLaMA\n",
    "\n",
    "参考文献：\n",
    "- [手写 transformer decoder（CausalLM）](https://bruceyuan.com/hands-on-code/hands-on-causallm-decoder.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入相关需要的包\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleDecoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, nums_head, dropout=0.1):\n",
    "        super(SimpleDecoder, self).__init__()\n",
    "        self.nums_head = nums_head\n",
    "        self.head_dim = hidden_dim // nums_head\n",
    "        self.dropout_prob = dropout\n",
    "        \n",
    "        # MHA\n",
    "        self.layernorm_att = nn.LayerNorm(hidden_dim, eps=1e-6)\n",
    "        self.q_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.k_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.v_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.o_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.dropout_att = nn.Dropout(self.dropout_prob)\n",
    "\n",
    "        # FFN\n",
    "        self.up_proj = nn.Linear(hidden_dim, hidden_dim * 4) # 升维\n",
    "        self.down_proj = nn.Linear(hidden_dim * 4, hidden_dim) # 降维\n",
    "        self.layernorm_ffn = nn.LayerNorm(hidden_dim, eps=1e-6)\n",
    "        self.activation_ffn = nn.ReLU()\n",
    "        self.dropout_ffn = nn.Dropout(self.dropout_prob)\n",
    "\n",
    "    def attention_output(self, query, key, value, attention_mask=None):\n",
    "        att_weight = torch.matmul(query, key.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "        # attention mask 进行依次调整；变成 causal_attention\n",
    "        # Pad Mask：过滤无效填充符号，避免模型关注输入序列中的填充符号，形状是二维矩阵（[B, L]）\n",
    "        # Causal Mask：防止未来信息泄漏，确保生成时每个位置只能访问历史信息，形状是下三角矩阵（[L, L]）\n",
    "        if attention_mask is not None:\n",
    "            # 变成下三角矩阵\n",
    "            attention_mask = attention_mask.tril()\n",
    "        else:\n",
    "            # 人工构造一个下三角的 attention mask\n",
    "            attention_mask = torch.ones_like(att_weight).tril()\n",
    "        att_weight = att_weight.masked_fill(attention_mask == 0, float('-inf'))\n",
    "        print(att_weight)\n",
    "        att_weight = self.dropout_att(att_weight)\n",
    "        att_weight = torch.softmax(att_weight, dim=-1)\n",
    "        mid_output = torch.matmul(att_weight, value) # shape is (batch_size, nums_head, seq_len, head_dim)\n",
    "        mid_output = mid_output.transpose(1, 2).contiguous()\n",
    "        batch_size, seq_len, _, _ = mid_output.size()\n",
    "        mid_output = mid_output.view(batch_size, seq_len, -1) # shape is (batch_size, seq_len, hidden_dim)\n",
    "        output = self.o_proj(mid_output)\n",
    "        return output       \n",
    "\n",
    "    def attention_block(self, x, att_mask=None):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        query = self.q_proj(x).view(batch_size, seq_len, self.nums_head, -1).transpose(1, 2) # (batch_size, nums_head, seq_len, head_dim)\n",
    "        key = self.q_proj(x).view(batch_size, seq_len, self.nums_head, -1).transpose(1, 2) # (batch_size, nums_head, seq_len, head_dim)\n",
    "        value = self.q_proj(x).view(batch_size, seq_len, self.nums_head, -1).transpose(1, 2) # (batch_size, nums_head, seq_len, head_dim)\n",
    "        output = self.attention_output(query, key, value, attention_mask=att_mask)\n",
    "        return self.layernorm_att(x + output)\n",
    "    \n",
    "    def ffn_block(self, x):\n",
    "        up = self.up_proj(x)\n",
    "        up = self.activation_ffn(up)\n",
    "        down = self.down_proj(up)\n",
    "        down = self.dropout_ffn(down)\n",
    "        return self.layernorm_ffn(x + down)\n",
    "    \n",
    "    def forward(self, x, att_mask=None):\n",
    "        # x shape is (batch_size, seq_len, hidden_dim)\n",
    "        # att_mask 一般是指经过tokenizer后返回的mask结果，表示哪些样本需要被忽略\n",
    "        att_output = self.attention_block(x, att_mask=att_mask)\n",
    "        ffn_output = self.ffn_block(att_output)\n",
    "        return ffn_output\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, layer_nums, emb_size=12, hidden_dim=64, nums_head=8, dropout=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layer_nums = layer_nums\n",
    "        self.emb_size = emb_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.nums_head = nums_head\n",
    "        self.dropout_prob = dropout\n",
    "        self.layer_list = nn.ModuleList(\n",
    "            [\n",
    "                SimpleDecoder(self.hidden_dim, self.nums_head, self.dropout_prob) for _ in range(layer_nums)\n",
    "            ]\n",
    "        )\n",
    "        self.emb = nn.Embedding(self.emb_size, self.hidden_dim)\n",
    "        self.out = nn.Linear(self.hidden_dim, self.emb_size)\n",
    "\n",
    "    def forward(self, x, att_mask=None):\n",
    "        x = self.emb(x)\n",
    "        for i, l in enumerate(self.layer_list):\n",
    "            x = l(x, att_mask=att_mask)\n",
    "        output = self.out(x)\n",
    "        return torch.softmax(output, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试\n",
    "x = torch.randint(low=0, high=12, size=(3, 4))\n",
    "net = Decoder(5)\n",
    "mask = (\n",
    "    torch.tensor([[1, 1, 1, 1], [1, 1, 0, 0], [1, 1, 1, 0]])\n",
    "    .unsqueeze(1)\n",
    "    .unsqueeze(2)\n",
    "    .repeat(1, 8, 4, 1)\n",
    ")\n",
    "print(x.size())\n",
    "print(mask.size())\n",
    "net(x, mask).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
