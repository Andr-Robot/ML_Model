{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 描述\n",
    "基于pytorch实现attention    \n",
    "\n",
    "参考文献：    \n",
    "- [手写 Self-Attention 的四重境界，从 self-attention 到 multi-head self-attention](https://bruceyuan.com/hands-on-code/from-self-attention-to-multi-head-self-attention.html#%E7%AC%AC%E5%9B%9B%E9%87%8D-multi-head-self-attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttentionV1(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(SelfAttentionV1, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.query_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.key_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.value_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, embedding_dim)\n",
    "        Q = self.query_proj(x) # (batch_size, seq_len, hidden_dim)\n",
    "        K = self.key_proj(x) # (batch_size, seq_len, hidden_dim)\n",
    "        V = self.value_proj(x) # (batch_size, seq_len, hidden_dim)\n",
    "\n",
    "        # Attention weights\n",
    "        # K 经过转置之后 shape: (batch_size, hidden_dim, seq_len)\n",
    "        attention_value = torch.matmul(Q, K.transpose(-1, -2))\n",
    "        # attention_weight shape: (batch_size, seq_len, seq_len)\n",
    "        attention_weight = torch.softmax(attention_value / math.sqrt(self.hidden_dim), dim=-1)\n",
    "        # output shape: (batch_size, seq_len, hidden_dim)\n",
    "        output = torch.matmul(attention_weight, V)\n",
    "        return output\n",
    "    \n",
    "X = torch.rand(3, 2, 4)\n",
    "print(X)\n",
    "net = SelfAttentionV1(4)\n",
    "ret = net(X)\n",
    "print(ret)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionV2(nn.Module):\n",
    "    # 将Q、K、V合并到一个大的矩阵中，然后一次性完成计算\n",
    "    def __init__(self, dim):\n",
    "        super(SelfAttentionV2, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.proj = nn.Linear(dim, 3 * dim)\n",
    "        self.output_proj = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, embedding_dim)\n",
    "        QKV = self.proj(x) # (batch_size, seq_len, 3 * dim)\n",
    "        # 分解Q、K、V\n",
    "        Q, K, V = torch.split(QKV, self.dim, dim=-1)\n",
    "        # Q, K, V shape: (batch_size, seq_len, dim)\n",
    "        att_weight = torch.softmax(torch.matmul(\n",
    "            Q, K.transpose(-1, -2)) / math.sqrt(self.dim), dim=-1\n",
    "        )\n",
    "        output = self.output_proj(torch.matmul(att_weight, V))\n",
    "        return output\n",
    "    \n",
    "X = torch.rand(3, 2, 4)\n",
    "print(X)\n",
    "net = SelfAttentionV2(4)\n",
    "ret = net(X)\n",
    "print(ret)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionV3(nn.Module):\n",
    "    # 加入dropout 和 mask\n",
    "    def __init__(self, dim, dropout=0.1):\n",
    "        super(SelfAttentionV3, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.proj = nn.Linear(dim, 3 * dim)\n",
    "        self.att_drop = nn.Dropout(dropout)\n",
    "        self.output_proj = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x, att_mask=None):\n",
    "        QKV = self.proj(x)\n",
    "        Q, K, V = torch.split(QKV, self.dim, dim=-1)\n",
    "        att_weight = torch.matmul(Q, K.transpose(-1, -2)) / math.sqrt(self.dim)\n",
    "        if att_mask != None:\n",
    "            # 给att_weigh添加一个极小值\n",
    "            att_weight = att_weight.masked_fill(att_mask == 0, float(\"-1e20\"))\n",
    "        att_weight = torch.softmax(att_weight, dim=-1)\n",
    "        att_weight = self.att_drop(att_weight)\n",
    "        output = self.output_proj(torch.matmul(att_weight, V))\n",
    "        return output\n",
    "    \n",
    "X = torch.rand(3, 4, 2)\n",
    "print(X)\n",
    "mask = torch.tensor(\n",
    "    [\n",
    "        [1, 1, 1, 0],\n",
    "        [1, 1, 0, 0],\n",
    "        [1, 0, 0, 0]\n",
    "    ]\n",
    ")\n",
    "print(mask.shape)\n",
    "x_mask = mask.unsqueeze(dim=1).repeat(1, 4, 1)\n",
    "print(x_mask)\n",
    "print(x_mask.shape)\n",
    "net = SelfAttentionV3(2)\n",
    "ret = net(X, x_mask)\n",
    "print(ret)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim, nums_head, dropout=0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.nums_head = nums_head\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.head_dim = hidden_dim // nums_head\n",
    "\n",
    "        self.q_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.k_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.v_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.att_dropout = nn.Dropout(dropout)\n",
    "        self.out_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, x, att_mask=None):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        Q = self.q_proj(x)\n",
    "        K = self.k_proj(x)\n",
    "        V = self.v_proj(x) # (batch_size, seq_len, embedding_dim)\n",
    "\n",
    "        # (batch_size, seq_len, embedding_dim) => (batch_size, head_num, seq_len, head_dim)\n",
    "        q_state = Q.view(batch_size, seq_len, self.nums_head, self.head_dim).transpose(1, 2)\n",
    "        k_state = K.view(batch_size, seq_len, self.nums_head, self.head_dim).transpose(1, 2)\n",
    "        v_state = V.view(batch_size, seq_len, self.nums_head, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # shape: (batch_size, head_num, seq_len, seq_len)\n",
    "        att_weight = q_state @ k_state.transpose(-1, -2) / math.sqrt(self.head_dim)\n",
    "        print(att_weight.shape)\n",
    "        if att_mask != None:\n",
    "            att_weight = att_weight.masked_fill(\n",
    "                att_weight == 0, float(\"-1e20\")\n",
    "            )\n",
    "        # 在最后一维做softmax\n",
    "        att_weight = torch.softmax(att_weight, dim=3)\n",
    "        att_weight = self.att_dropout(att_weight)\n",
    "        output_mid = att_weight @ v_state # (batch_size, head_num, seq_len, head_dim)\n",
    "        \n",
    "        # (batch_size, head_num, seq_len, head_dim) => (batch_size, seq_len, head_num, head_dim)\n",
    "        # 这里的 contiguous() 是相当于返回一个连续内存的 tensor，因为后面用的是view，view只能在连续的内存上操作\n",
    "        output_mid = output_mid.transpose(1, 2).contiguous()\n",
    "        output = output_mid.view(batch_size, seq_len, -1)\n",
    "        output = self.out_proj(output)\n",
    "        return output\n",
    "        \n",
    "\n",
    "\n",
    "X = torch.rand(3, 4, 128)\n",
    "mask = torch.tensor(\n",
    "    [\n",
    "        [1, 1, 1, 0],\n",
    "        [1, 1, 0, 0],\n",
    "        [1, 0, 0, 0]\n",
    "    ]\n",
    ")\n",
    "print(mask.shape)\n",
    "x_mask = mask.unsqueeze(dim=1).unsqueeze(dim=2).repeat(1, 8, 4, 1)\n",
    "print(x_mask.shape)\n",
    "net = MultiHeadAttention(128, 8)\n",
    "ret = net(X, x_mask)\n",
    "print(ret)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
