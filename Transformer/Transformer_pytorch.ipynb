{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 描述\n",
    "基于pytorch实现Transformer模型    \n",
    "代码参考：    \n",
    "- [introduction-to-transformers kaggle](https://www.kaggle.com/code/alejopaullier/introduction-to-transformers)\n",
    "- [Attention is All You Need论文复现代码 GitHub](https://github.com/tian-guo-guo/tian-guo-guo.github.io/blob/179a0646232128b39ea12156ff292b394e9f24a3/_posts/2019-09-15-6%20-%20Attention%20is%20All%20You%20Need%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0%E4%BB%A3%E7%A0%81.md)\n",
    "\n",
    "所需三方库版本：    \n",
    "- python: 3.11.0\n",
    "- torch: 2.2.0\n",
    "- torchtext: 0.17.0\n",
    "- spacy: 3.7.5\n",
    "- numpy: 1.25.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer模型结构\n",
    "![Transformer Architecture](https://raw.githubusercontent.com/Andr-Robot/iMarkdownPhotos/refs/heads/master/Res/ml/transformer_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 导入必要的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 多头注意力机制（Multi-Head Attention）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        \"\"\"\n",
    "        MultiHeadAttention mechanism. \n",
    "        The input of the MultiHeadAttention mechanism is an embedding (or sequence of embeddings). \n",
    "        The embeddings are split into different parts and each part is fed into a head.\n",
    "        args:\n",
    "            embed_size (int): the size of the embedding\n",
    "            heads (int): the number of heads you wish to create\n",
    "        \"\"\"\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.embed_size = embed_size # 512 in Transformer\n",
    "        self.heads = heads # 8 in Transformer\n",
    "        self.head_dim = embed_size // heads # 64 in Transformer\n",
    "\n",
    "        assert (\n",
    "            self.head_dim * heads == embed_size\n",
    "        ), \"Embedding size needs to be divisible by heads\"\n",
    "        \n",
    "        # Embedding 会被投影到Query, Key and Value\n",
    "        # 先投影到embed_size, 然后再分成N个维度为head_dim的部分\n",
    "        self.values = nn.Linear(embed_size, embed_size)\n",
    "        self.keys = nn.Linear(embed_size, embed_size)\n",
    "        self.queries = nn.Linear(embed_size, embed_size)\n",
    "        self.fc_out = nn.Linear(embed_size, embed_size)\n",
    "\n",
    "    def forward(self, values, keys, query, mask):\n",
    "        # 定义前向传播方法，接收value, key, query和mask作为输入\n",
    "        # Values, Keys and Queries have size: (batch_size, sequence_len, embedding_size)\n",
    "        batch_size = query.shape[0]# Get number of training examples/batch size.\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "        # === Pass through Linear Layer ===\n",
    "        \"\"\"\n",
    "        step1: 线性变换\n",
    "        对 Q、K、V 进行线性变换，将其投影到多个头的维度\n",
    "        变换后的形状为：(batch_size, sequence_len, embed_size) -> (batch_size, sequence_len, heads, head_dim)\n",
    "        \"\"\"\n",
    "        values = self.values(values)  # (batch_size, value_len, embed_size)\n",
    "        keys = self.keys(keys)  # (batch_size, key_len, embed_size)\n",
    "        queries = self.queries(query)  # (batch_size, query_len, embed_size)\n",
    "\n",
    "        # Split the embedding into self.heads different pieces\n",
    "        values = values.reshape(batch_size, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(batch_size, key_len, self.heads, self.head_dim)\n",
    "        queries = queries.reshape(batch_size, query_len, self.heads, self.head_dim)\n",
    "\n",
    "        # Einsum does matrix mult. for query*keys for each training example\n",
    "        # with every other training example, don't be confused by einsum\n",
    "        # it's just how I like doing matrix multiplication & bmm\n",
    "\n",
    "        # einsum函数是NumPy和PyTorch中用于执行爱因斯坦求和约定的函数。它可以简洁地表示复杂的多维数组操作。\n",
    "        \"\"\"\n",
    "        step2: 计算注意力得分\n",
    "        使用 Einsum 函数(BMM)计算 Q 和 K 的点积，得到注意力分数\n",
    "        \"\"\"\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "        # queries shape: (batch_size, query_len, heads, heads_dim),\n",
    "        # keys shape: (batch_size, key_len, heads, heads_dim)\n",
    "        # energy: (batch_size, heads, query_len, key_len)\n",
    "\n",
    "        # Mask padded indices so their weights become 0\n",
    "        # 如果提供了掩蔽矩阵，将其应用到得分上，将掩蔽位置处的得分设置为负无穷大\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        # Normalize energy values similarly to seq2seq + attention\n",
    "        # so that they sum to 1. Also divide by scaling factor for\n",
    "        # better stability\n",
    "        \"\"\"\n",
    "        step3: 计算注意力权重\n",
    "        归一化注意力得分，使其和为1，并除以缩放因子以提高稳定性\n",
    "        \"\"\"\n",
    "        attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3) \n",
    "        # attention shape: (batch_size, heads, query_len, key_len)\n",
    "        # values shape: (batch_size, value_len, heads, heads_dim)\n",
    "        # out after matrix multiply: (batch_size, query_len, heads, head_dim), then\n",
    "        # we reshape and flatten the last two dimensions.\n",
    "        \"\"\"\n",
    "        step4: 计算加权和\n",
    "        使用注意力权重对V进行加权求和，并reshape结果\n",
    "        变换后的形状为：(batch_size, query_len, heads, head_dim) -> (batch_size, query_len, embed_size)\n",
    "        \"\"\"\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "            batch_size, query_len, self.heads * self.head_dim\n",
    "        )\n",
    "        # Linear layer doesn't modify the shape, final shape will be\n",
    "        # (batch_size, query_len, embed_size)\n",
    "        \"\"\"\n",
    "        step5: 输出层\n",
    "        通过最终的线性层输出结果\n",
    "        \"\"\"\n",
    "        out = self.fc_out(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Transformer Layer\n",
    "严格意义上应该算是Encoder Layer，因为Decoder Layer中会使用到这个部分，所以改成TransformerLayer，防止混淆   \n",
    "1. Multi-Head Attention\n",
    "2. Add & Norm\n",
    "3. Feed Forward\n",
    "4. Add & Norm again\n",
    "\n",
    "![encoder_layer](https://raw.githubusercontent.com/Andr-Robot/iMarkdownPhotos/refs/heads/master/Res/ml/encoder_layer.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLayer(nn.Module):\n",
    "    \"\"\"TransformerLayer 层的实现代码包含了多头注意力机制（Multi-Head Attention）和前馈神经网络（FeedForward Network, FFN）\"\"\"\n",
    "    def __init__(self, embed_size, heads, dropout, forward_expansion=4):\n",
    "        \"\"\"\n",
    "        TransformerLayer 层的实现代码包含了多头注意力机制（Multi-Head Attention）和前馈神经网络（FeedForward Network, FFN）\n",
    "        args:\n",
    "            embed_size (int): embedding的大小\n",
    "            heads (int): 多头注意力机制的头数\n",
    "            dropout (float): dropout的概率，用于防止过拟合\n",
    "            forward_expansion (int): 前馈网络中隐藏层神经元的扩展倍数\n",
    "        \"\"\"\n",
    "        super(TransformerLayer, self).__init__()\n",
    "        # 用于计算注意力权重\n",
    "        self.attention = MultiHeadAttention(embed_size, heads) \n",
    "        # Layer Normalization 层，用于稳定训练过程\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "        # 前馈神经网络，由两个线性层和 ReLU 激活函数组成。\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(forward_expansion * embed_size, embed_size),\n",
    "        )\n",
    "        # Dropout 层，用于随机丢弃部分神经元，防止过拟合\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, value, key, query, mask):\n",
    "        # 定义前向传播方法，接收value, key, query和mask作为输入\n",
    "        # Values, Keys and Queries have size: (batch_size, query_len, embedding_size)\n",
    "        attention = self.attention(value, key, query, mask) # attention shape: (batch_size, query_len, embedding_size)\n",
    "        # Add skip connection, run through normalization and finally dropout\n",
    "        # 将注意力输出与输入 query 相加（残差连接）\n",
    "        x = self.dropout(self.norm1(attention + query)) # x shape: (batch_size, query_len, embedding_size)\n",
    "        # 将归一化后的结果输入到前馈网络中\n",
    "        forward = self.feed_forward(x) # forward shape: (batch_size, query_len, embedding_size)\n",
    "        # 将前馈网络的输出与输入 x 相加（残差连接）\n",
    "        out = self.dropout(self.norm2(forward + x)) # out shape: (batch_size, query_len, embedding_size)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "1. Embedding\n",
    "2. Positional Encoding\n",
    "3. Transformer Block\n",
    "\n",
    "![encoder](https://raw.githubusercontent.com/Andr-Robot/iMarkdownPhotos/refs/heads/master/Res/ml/encoder.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, src_vocab_size, embed_size, num_layers, heads,\n",
    "        device, forward_expansion, dropout, max_length):\n",
    "        \"\"\"\n",
    "        Transformer 编码器的初始化方法。\n",
    "\n",
    "        Args:\n",
    "            src_vocab_size (int): 源语言词汇表的大小。\n",
    "            embed_size (int): 词嵌入的维度。\n",
    "            num_layers (int): 编码器中 Transformer 层的数量。\n",
    "            heads (int): 多头注意力机制的头数。\n",
    "            device (str): 模型运行的设备，例如 \"cuda\" 或 \"cpu\"。\n",
    "            forward_expansion (int): 前馈网络中隐藏层的扩展倍数。\n",
    "            dropout (float): Dropout 概率。\n",
    "            max_length (int): 输入序列的最大长度。\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embed_size = embed_size # size of the input embedding\n",
    "        self.device = device # either \"cuda\" or \"cpu\"\n",
    "        # Lookup table with an embedding for each word in the vocabulary\n",
    "        # 词嵌入层：将输入的单词索引映射为高维向量\n",
    "        self.word_embedding = nn.Embedding(src_vocab_size, embed_size) \n",
    "        # Lookup table with a positional embedding for each word in the sequence\n",
    "        # 位置嵌入层：为序列中的每个位置生成嵌入向量\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "        # 多层 Transformer 层：堆叠 num_layers 个 TransformerLayer\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerLayer(\n",
    "                    embed_size,\n",
    "                    heads,\n",
    "                    dropout=dropout,\n",
    "                    forward_expansion=forward_expansion,\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        # Dropout 层：用于随机丢弃部分神经元，防止过拟合\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        前向传播方法。\n",
    "\n",
    "        args:\n",
    "            x (torch.Tensor): 输入序列，形状为 (batch_size, sequence_length)，其中每个元素是单词的索引。\n",
    "            mask (torch.Tensor): 用于屏蔽无效位置的掩码（例如填充位置）。\n",
    "\n",
    "        return:\n",
    "            out (torch.Tensor): 编码器的输出，形状为 (batch_size, sequence_length, embed_size)。\n",
    "        \"\"\"\n",
    "        batch_size, seq_length = x.shape\n",
    "        # positions is an arange from (0,seq_len), e.g: torch.tensor([[0,1,2,...,N], [0,1,2,...,N], ..., [0,1,2,...,N]])\n",
    "        # 生成位置信息：创建一个从 0 到 seq_length-1 的位置索引\n",
    "        # 例如，如果 seq_length=5，则 positions 为 [[0, 1, 2, 3, 4], [0, 1, 2, 3, 4], ...]\n",
    "        positions = torch.arange(0, seq_length).expand(batch_size, seq_length).to(self.device)\n",
    "        # 词嵌入 + 位置嵌入：将词嵌入和位置嵌入相加，得到最终的输入表示\n",
    "        # 使用 Dropout 进行随机丢弃，防止过拟合\n",
    "        x_x = self.word_embedding(x)\n",
    "        p_p = self.position_embedding(positions)\n",
    "        out = self.dropout((x_x + p_p))\n",
    "        # In the Encoder the query, key, value are all the same, in the\n",
    "        # decoder this will change. This might look a bit odd in this case.\n",
    "        # 通过多层 Transformer 层进行编码\n",
    "        for layer in self.layers:\n",
    "            # 在编码器中，query、key 和 value 都是相同的（即 out）\n",
    "            out = layer(out, out, out, mask)\n",
    "        # output shape: torch.Size([batch_size, sequence_length, embedding_size])\n",
    "        # 返回编码器的输出\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Layer\n",
    "1. Masked Multi-Head Attention\n",
    "2. Add & Norm\n",
    "3. Multi-Head Attention\n",
    "4. Add & Norm\n",
    "5. Feed Forward\n",
    "6. Add & Norm\n",
    "\n",
    "![decoder layer](https://raw.githubusercontent.com/Andr-Robot/iMarkdownPhotos/refs/heads/master/Res/ml/decoder_layer.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"Decoder 层的实现\"\"\"\n",
    "    def __init__(self, embed_size, heads, forward_expansion, dropout, device):\n",
    "        \"\"\"\n",
    "        Decoder 层的初始化方法。\n",
    "\n",
    "        Args:\n",
    "            embed_size (int): 输入嵌入的维度。\n",
    "            heads (int): 多头注意力机制的头数。\n",
    "            forward_expansion (int): 前馈网络中隐藏层的扩展倍数。\n",
    "            dropout (float): Dropout 概率。\n",
    "            device (str): 模型运行的设备，例如 \"cuda\" 或 \"cpu\"。\n",
    "        \"\"\"\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        # Layer Normalization 层，用于稳定训练过程\n",
    "        self.norm = nn.LayerNorm(embed_size)\n",
    "        # 多头注意力机制，用于计算目标序列的自注意力\n",
    "        self.attention = MultiHeadAttention(embed_size, heads=heads)\n",
    "        # Transformer 层，用于结合编码器的输出和目标序列的表示\n",
    "        self.transformer_block = TransformerLayer(\n",
    "            embed_size, heads, dropout, forward_expansion\n",
    "        )\n",
    "        # Dropout 层，用于随机丢弃部分神经元，防止过拟合\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, value, key, src_mask, trg_mask):\n",
    "        \"\"\"\n",
    "        前向传播方法\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): 目标序列的输入，形状为 (batch_size, target_sequence_len, embed_size)。\n",
    "            value (torch.Tensor): 编码器提取的 value，形状为 (batch_size, source_sequence_len, embed_size)。\n",
    "            key (torch.Tensor): 编码器提取的 key，形状为 (batch_size, source_sequence_len, embed_size)。\n",
    "            src_mask (torch.Tensor): 源序列的掩码，用于屏蔽填充位置，形状为 (batch_size, 1, source_sequence_len)。\n",
    "            trg_mask (torch.Tensor): 目标序列的掩码，用于屏蔽未来位置，形状为 (batch_size, target_sequence_len, target_sequence_len)。\n",
    "\n",
    "        Returns:\n",
    "            out (torch.Tensor): 解码器层的输出，形状为 (batch_size, target_sequence_len, embed_size)。\n",
    "        \"\"\"\n",
    "        # 计算目标序列的自注意力\n",
    "        attention = self.attention(x, x, x, trg_mask)\n",
    "        # 残差连接 + Layer Normalization + Dropout\n",
    "        query = self.dropout(self.norm(attention + x))\n",
    "        # 通过 Transformer 层，结合编码器的输出和目标序列的表示\n",
    "        out = self.transformer_block(value, key, query, src_mask)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "1. Output Embedding\n",
    "2. Decoder Block\n",
    "3. Linear\n",
    "4. Softmax\n",
    "\n",
    "![decoder](https://raw.githubusercontent.com/Andr-Robot/iMarkdownPhotos/refs/heads/master/Res/ml/decoder.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"解码器的实现\"\"\"\n",
    "    def __init__(self, trg_vocab_size, embed_size, num_layers, heads, forward_expansion,\n",
    "        dropout, device, max_length):\n",
    "        \"\"\"\n",
    "        Decoder 的初始化方法。\n",
    "\n",
    "        Args:\n",
    "            trg_vocab_size (int): 目标词汇表的大小。\n",
    "            embed_size (int): 输入嵌入的维度。\n",
    "            num_layers (int): 解码器中 DecoderLayer 的数量。\n",
    "            heads (int): 多头注意力机制的头数。\n",
    "            forward_expansion (int): 前馈网络中隐藏层的扩展倍数。\n",
    "            dropout (float): Dropout 概率。\n",
    "            device (str): 模型运行的设备，例如 \"cuda\" 或 \"cpu\"。\n",
    "            max_length (int): 目标序列的最大长度。\n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "        self.device = device\n",
    "        #=== For each token in target vocab there is a token embedding ===\n",
    "        # 词嵌入层：将目标序列的单词索引映射为高维向量# \n",
    "        self.word_embedding = nn.Embedding(trg_vocab_size, embed_size) \n",
    "        # 位置嵌入层：为序列中的每个位置生成嵌入向量\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "        # 多层 DecoderLayer：堆叠 num_layers 个 DecoderLayer\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                DecoderLayer(embed_size, heads, forward_expansion, dropout, device)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        # 线性层：将解码器的输出映射为目标词汇表大小的概率分布\n",
    "        self.fc_out = nn.Linear(embed_size, trg_vocab_size)\n",
    "        # Dropout 层：随机丢弃部分神经元，防止过拟合\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_out, src_mask, trg_mask):\n",
    "        \"\"\"\n",
    "        前向传播方法\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): 目标序列的输入，形状为 (batch_size, target_sequence_len)。\n",
    "            enc_out (torch.Tensor): 编码器的输出，形状为 (batch_size, source_sequence_len, embed_size)。\n",
    "            src_mask (torch.Tensor): 源序列的掩码，形状为 (batch_size, 1, source_sequence_len)。\n",
    "            trg_mask (torch.Tensor): 目标序列的掩码，形状为 (batch_size, target_sequence_len, target_sequence_len)。\n",
    "\n",
    "        Returns:\n",
    "            out (torch.Tensor): 解码器的输出，形状为 (batch_size, target_sequence_len, trg_vocab_size)。\n",
    "        \"\"\"\n",
    "        batch_size, seq_length = x.shape # x shape: (batch_size, target_sequence_len)\n",
    "        # positions is an arange from (0,seq_len), e.g: torch.tensor([[0,1,2,...,N], [0,1,2,...,N], ..., [0,1,2,...,N]])\n",
    "        # 生成位置信息：创建一个从 0 到 seq_length-1 的位置索引\n",
    "        # 例如，如果 seq_length=5，则 positions 为 [[0,1,2,3,4], [0,1,2,3,4], ..., [0,1,2,3,4]]\n",
    "        positions = torch.arange(0, seq_length).expand(batch_size, seq_length).to(self.device) # positions shape: (batch_size, target_sequence_len)\n",
    "        # 词嵌入 + 位置嵌入：将词嵌入和位置嵌入相加，得到目标序列的初始表示\n",
    "        # 使用 Dropout 进行随机丢弃\n",
    "        x_x = self.word_embedding(x)\n",
    "        p_p = self.position_embedding(positions)\n",
    "        x = self.dropout((x_x + p_p))\n",
    "\n",
    "        # 通过多层 DecoderLayer\n",
    "        for layer in self.layers:\n",
    "            # 在 DecoderLayer 中，目标序列的表示与编码器的输出结合\n",
    "            x = layer(x, enc_out, enc_out, src_mask, trg_mask)\n",
    "        # 通过线性层，将解码器的输出映射为目标词汇表大小的概率分布\n",
    "        out = self.fc_out(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"\"\"Transformer 模型的实现\"\"\"\n",
    "    def __init__(self, src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx, embed_size=512,\n",
    "                 num_layers=6, forward_expansion=4, heads=8, dropout=0, device=\"cpu\", max_length=100):\n",
    "        \"\"\"\n",
    "        Transformer 的初始化方法。\n",
    "\n",
    "        Args:\n",
    "            src_vocab_size (int): 源词汇表的大小。\n",
    "            trg_vocab_size (int): 目标词汇表的大小。\n",
    "            src_pad_idx (int): 源序列中的填充索引。\n",
    "            trg_pad_idx (int): 目标序列中的填充索引。\n",
    "            embed_size (int): 输入嵌入的维度。\n",
    "            num_layers (int): 编码器和解码器中 EncoderLayer 和 DecoderLayer 的数量。\n",
    "            heads (int): 多头注意力机制中的头数。\n",
    "            forward_expansion (int): 前馈网络中隐藏层的扩展倍数。\n",
    "            dropout (float): Dropout 概率。\n",
    "            device (str): 模型运行的设备，例如 \"cuda\" 或 \"cpu\"。\n",
    "            max_length (int): 输入序列的最大长度。\n",
    "        \"\"\"\n",
    "        super(Transformer, self).__init__()\n",
    "        # === Encoder ===\n",
    "        self.encoder = Encoder(src_vocab_size, embed_size, num_layers, heads, device, forward_expansion, dropout, max_length)\n",
    "        # === Decoder ===\n",
    "        self.decoder = Decoder(trg_vocab_size, embed_size, num_layers, heads, forward_expansion, dropout, device, max_length)\n",
    "        # 填充索引\n",
    "        # 填充索引（如 <PAD>）用于表示填充的位置。\n",
    "        # 在词汇表中，填充索引通常被映射为一个特殊的 token（如 0 或 1）。\n",
    "        self.src_pad_idx = src_pad_idx # 源序列的填充索引\n",
    "        self.trg_pad_idx = trg_pad_idx # 目标序列的填充索引\n",
    "        self.device = device\n",
    "\n",
    "    def make_src_mask(self, src):\n",
    "        \"\"\"\n",
    "        生成源序列的掩码，由于屏蔽填充位置（padding）\n",
    "\n",
    "        Args:\n",
    "            src (torch.Tensor): 源序列，形状为 (batch_size, src_sequence_len)。\n",
    "\n",
    "        Returns:\n",
    "            src_mask (torch.Tensor): 源序列的掩码，形状为 (batch_size, 1, 1, src_sequence_len)，其中填充位置为 False，有效位置为 True。\n",
    "        \"\"\"\n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        # (N, 1, 1, src_len)\n",
    "        return src_mask.to(self.device)\n",
    "\n",
    "    def make_trg_mask(self, trg):\n",
    "        \"\"\"\n",
    "        生成目标序列的掩码，用于屏蔽未来位置。\n",
    "\n",
    "        Args:\n",
    "            trg (torch.Tensor): 目标序列，形状为 (batch_size, trg_sequence_len)。\n",
    "\n",
    "        Returns:\n",
    "            trg_mask (torch.Tensor): 目标序列的掩码，形状为 (batch_size, 1, trg_sequence_len, trg_sequence_len)，其中未来位置为 0，当前及之前位置为 1。\n",
    "        \n",
    "        Example: for a target of shape (batch_size=1, target_size=4)\n",
    "        tensor([[[[1., 0., 0., 0.],\n",
    "                  [1., 1., 0., 0.],\n",
    "                  [1., 1., 1., 0.],\n",
    "                  [1., 1., 1., 1.]]]])\n",
    "        \"\"\"\n",
    "        N, trg_len = trg.shape\n",
    "        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n",
    "            N, 1, trg_len, trg_len\n",
    "        )\n",
    "        return trg_mask.to(self.device)\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        \"\"\"\n",
    "        前向传播方法。\n",
    "\n",
    "        Args:\n",
    "            src (torch.Tensor): 源序列，形状为 (batch_size, src_sequence_len)。\n",
    "            trg (torch.Tensor): 目标序列，形状为 (batch_size, trg_sequence_len)。\n",
    "\n",
    "        Returns:\n",
    "            out (torch.Tensor): 模型的输出，形状为 (batch_size, trg_sequence_len, trg_vocab_size)。\n",
    "        \"\"\"\n",
    "        # 生成源序列掩码\n",
    "        src_mask = self.make_src_mask(src) # src_mask shape: (batch_size, 1, 1, src_sequence_len)\n",
    "        # 生成目标序列掩码\n",
    "        trg_mask = self.make_trg_mask(trg) # trg_mask shape: (batch_size, 1, trg_sequence_len, trg_sequence_len)\n",
    "        # 编码器输出\n",
    "        enc_src = self.encoder(src, src_mask) # enc_src shape: (batch_size, src_sequence_len, embed_size)\n",
    "        # 解码器输出\n",
    "        out = self.decoder(trg, enc_src, src_mask, trg_mask) # out shape: (batch_size, trg_sequence_len, trg_vocab_size)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 简单测试代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 9])\n",
      "Target shape: torch.Size([2, 8])\n",
      "Device available: cpu\n",
      "Output shape: torch.Size([2, 7, 10])\n",
      "Output: tensor([[[-0.0301,  0.0609, -0.5596, -0.5251, -0.4659,  0.5850, -0.4768,\n",
      "           0.3702,  1.9181,  0.5932],\n",
      "         [-1.1331, -0.2159, -0.3208, -0.9711,  0.2481, -0.4936, -0.5312,\n",
      "           0.2812,  2.0300,  0.3126],\n",
      "         [-0.2906,  0.8850, -0.3910, -1.1984,  1.0430,  0.4958, -0.0374,\n",
      "           0.4621,  0.9843,  0.2634],\n",
      "         [ 0.2161,  0.2121,  0.5236, -0.9407,  0.3107, -0.2067,  0.4018,\n",
      "           1.5001,  0.2991,  1.1865],\n",
      "         [-0.2761, -0.1771, -0.9052, -1.4448,  0.2574, -0.0782,  0.5652,\n",
      "           1.0252,  0.6613,  1.4735],\n",
      "         [-0.4923,  0.5697,  0.3478, -0.3036,  0.4055,  0.6242,  0.2012,\n",
      "           1.1592,  1.2393,  1.0222],\n",
      "         [-0.6083, -0.5369, -0.3365, -0.5693,  0.6065, -0.7188, -0.8129,\n",
      "           0.8671,  0.2657,  1.2910]],\n",
      "\n",
      "        [[ 0.1963,  0.2490, -0.5154, -0.6422, -0.3840,  0.5401, -0.4969,\n",
      "           0.3716,  1.6671,  0.6228],\n",
      "         [ 0.1149, -0.1055, -0.7717, -1.6912, -0.1544, -0.2647, -0.1030,\n",
      "           0.9096,  0.6631,  0.9764],\n",
      "         [-0.3744,  0.3590, -0.5763, -0.8609,  0.3020, -0.2035, -0.1592,\n",
      "           0.2408,  0.7519,  0.5814],\n",
      "         [ 0.3342,  0.2438, -0.5516, -0.0156, -0.1716, -0.5916, -1.0750,\n",
      "           0.8471,  0.2051,  1.0752],\n",
      "         [-0.2969,  0.7895, -0.6252, -1.0023,  0.4463, -0.0656, -0.0365,\n",
      "           1.0932,  0.9137,  0.7658],\n",
      "         [-0.7177,  0.4922, -0.1152, -0.2068,  0.8707, -0.1376, -0.2718,\n",
      "          -0.1152,  1.7128,  0.2294],\n",
      "         [-0.7946, -0.3488,  0.3728, -1.1080,  0.3049, -0.9537, -0.4991,\n",
      "           0.8203,  0.4956,  1.3544]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    测试代码\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # 定义输入数据\n",
    "    x = torch.tensor([[1, 5, 6, 4, 3, 9, 5, 2, 0], [1, 8, 7, 3, 4, 5, 6, 7, 2]]).to(device)\n",
    "    trg = torch.tensor([[1, 7, 4, 3, 5, 9, 2, 0], [1, 5, 6, 2, 4, 7, 6, 2]]).to(device)\n",
    "    # 定义填充索引\n",
    "    src_pad_idx = 0 # index of the padding token in source vocabulary\n",
    "    trg_pad_idx = 0 # index of the padding token in target vocabulary\n",
    "    # 定义词汇表大小\n",
    "    src_vocab_size = 10 # number of unique tokens in source vocabulary\n",
    "    trg_vocab_size = 10 # number of unique tokens in target vocabulary\n",
    "    \n",
    "    print(f\"Input shape: {x.shape}\")\n",
    "    print(f\"Target shape: {trg.shape}\")\n",
    "    print(f\"Device available: {device}\")\n",
    "    # 初始化 Transformer 模型\n",
    "    model = Transformer(src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx, device=device).to(device)\n",
    "\n",
    "    out = model(x, trg[:, :-1])\n",
    "    print(f\"Output shape: {out.shape}\")\n",
    "    print(f\"Output: {out}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型训练\n",
    "使用PyTorch和torchtext库来加载和预处理WMT'14 English-German数据集，训练和评估一个简单的Transformer模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Epoch: 01 | Time: 1m 35s\n",
      "\tTrain Loss: 6.655\n",
      "\t Val. Loss: 5.483\n",
      "Epoch 2\n",
      "Epoch: 02 | Time: 1m 36s\n",
      "\tTrain Loss: 5.313\n",
      "\t Val. Loss: 4.942\n",
      "Epoch 3\n",
      "Epoch: 03 | Time: 1m 36s\n",
      "\tTrain Loss: 5.009\n",
      "\t Val. Loss: 4.752\n",
      "Epoch 4\n",
      "Epoch: 04 | Time: 1m 36s\n",
      "\tTrain Loss: 4.813\n",
      "\t Val. Loss: 4.561\n",
      "Epoch 5\n",
      "Epoch: 05 | Time: 1m 34s\n",
      "\tTrain Loss: 4.646\n",
      "\t Val. Loss: 4.400\n",
      "Epoch 6\n",
      "Epoch: 06 | Time: 1m 31s\n",
      "\tTrain Loss: 4.512\n",
      "\t Val. Loss: 4.277\n",
      "Epoch 7\n",
      "Epoch: 07 | Time: 1m 31s\n",
      "\tTrain Loss: 4.406\n",
      "\t Val. Loss: 4.186\n",
      "Epoch 8\n",
      "Epoch: 08 | Time: 1m 31s\n",
      "\tTrain Loss: 4.328\n",
      "\t Val. Loss: 4.125\n",
      "Epoch 9\n",
      "Epoch: 09 | Time: 1m 31s\n",
      "\tTrain Loss: 4.266\n",
      "\t Val. Loss: 4.082\n",
      "Epoch 10\n",
      "Epoch: 10 | Time: 1m 32s\n",
      "\tTrain Loss: 4.211\n",
      "\t Val. Loss: 4.028\n",
      "Test Loss: 4.030\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import Multi30k\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# 定义字段\n",
    "SRC_LANGUAGE = 'de'\n",
    "TGT_LANGUAGE = 'en'\n",
    "\n",
    "token_transform = {}\n",
    "vocab_transform = {}\n",
    "\n",
    "# 使用 spacy 进行分词\n",
    "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='de_core_news_sm')\n",
    "token_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "\n",
    "# 构建词汇表\n",
    "def yield_tokens(data_iter, language):\n",
    "    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n",
    "    for data_sample in data_iter:\n",
    "        yield token_transform[language](data_sample[language_index[language]])\n",
    "\n",
    "\"\"\"\n",
    "这里会无法下载数据，可以手动下载数据集，放到 ~/.torchtext/cache/Multi30k/ 文件夹下\n",
    "\"\"\"\n",
    "# We need to modify the URLs for the dataset since the links to the original dataset are broken\n",
    "# Refer to https://github.com/pytorch/text/issues/1756#issuecomment-1163664163 for more info\n",
    "# multi30k.URL[\"train\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\"\n",
    "# multi30k.URL[\"valid\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\"\n",
    "# multi30k.URL[\"test\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/mmt16_task1_test.tar.gz\"\n",
    "\n",
    "# 指定本地数据集路径\n",
    "local_data_path = '/Users/leo/.torchtext/cache/Multi30k'  # 替换为你的数据集路径\n",
    "\n",
    "# 确保数据集文件存在\n",
    "assert os.path.exists(local_data_path), \"数据集路径不存在，请检查路径是否正确\"\n",
    "\n",
    "# 加载本地数据集\n",
    "train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE), root=local_data_path)\n",
    "valid_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE), root=local_data_path)\n",
    "\n",
    "\"\"\"\n",
    "https://github.com/pytorch/text/issues/2221\n",
    "mmt16_task1_test.tar.gz torchtext有bug，会导致加载失败\n",
    "UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 37: invalid start byte\n",
    "解决方案：\n",
    "将mmt16_task1_test.tar.gz解压，并将文件移动到~/.torchtext/cache/Multi30k/datasets/Multi30k/目录下\n",
    "1. tar -zxvf mmt16_task1_test.tar.gz\n",
    "2. mv test.* datasets/Multi30k/\n",
    "3. cp mmt16_task1_test.tar.gz datasets/Multi30k/\n",
    "\"\"\"\n",
    "\n",
    "test_iter = Multi30k(split='test', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE), root=local_data_path)\n",
    "\n",
    "\n",
    "vocab_transform[SRC_LANGUAGE] = build_vocab_from_iterator(yield_tokens(train_iter, SRC_LANGUAGE), min_freq=2, specials=[\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"])\n",
    "vocab_transform[TGT_LANGUAGE] = build_vocab_from_iterator(yield_tokens(train_iter, TGT_LANGUAGE), min_freq=2, specials=[\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"])\n",
    "\n",
    "for vocab in vocab_transform.values():\n",
    "    vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "# 定义常量\n",
    "SRC_PAD_IDX = vocab_transform[SRC_LANGUAGE][\"<pad>\"]\n",
    "TGT_PAD_IDX = vocab_transform[TGT_LANGUAGE][\"<pad>\"]\n",
    "BATCH_SIZE = 128\n",
    "MAX_LENGTH = 128\n",
    "# DROUPOUT = 0.3\n",
    "EMBEDDING_SIZE = 256\n",
    "NUM_LAYERS = 3\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 数据处理函数\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for src_sample, tgt_sample in batch:\n",
    "        src_batch.append(torch.tensor([vocab_transform[SRC_LANGUAGE][\"<bos>\"]] + vocab_transform[SRC_LANGUAGE](token_transform[SRC_LANGUAGE](src_sample)) + [vocab_transform[SRC_LANGUAGE][\"<eos>\"]], dtype=torch.long))\n",
    "        tgt_batch.append(torch.tensor([vocab_transform[TGT_LANGUAGE][\"<bos>\"]] + vocab_transform[TGT_LANGUAGE](token_transform[TGT_LANGUAGE](tgt_sample)) + [vocab_transform[TGT_LANGUAGE][\"<eos>\"]], dtype=torch.long))\n",
    "    src_batch = nn.utils.rnn.pad_sequence(src_batch, padding_value=SRC_PAD_IDX)\n",
    "    tgt_batch = nn.utils.rnn.pad_sequence(tgt_batch, padding_value=TGT_PAD_IDX)\n",
    "    return src_batch, tgt_batch\n",
    "\n",
    "# 创建数据加载器\n",
    "\"\"\"\n",
    "Transformer 的输入格式：\n",
    "    Transformer 模型的输入通常是一个形状为 (批次大小, 序列长度) 的张量。\n",
    "    例如，src 的形状应为 (batch_size, src_seq_len)，trg 的形状应为 (batch_size, trg_seq_len)。\n",
    "DataLoader 的输出格式：\n",
    "    默认情况下，DataLoader 返回的批次数据形状是 (序列长度, 批次大小)。\n",
    "    例如，src_batch 的形状为 (src_seq_len, batch_size)，trg_batch 的形状为 (trg_seq_len, batch_size)。\n",
    "\"\"\"\n",
    "train_dataloader = DataLoader(list(train_iter), batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "valid_dataloader = DataLoader(list(valid_iter), batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(list(test_iter), batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "# 初始化模型\n",
    "INPUT_DIM = len(vocab_transform[SRC_LANGUAGE])\n",
    "OUTPUT_DIM = len(vocab_transform[TGT_LANGUAGE])\n",
    "\n",
    "model = Transformer(INPUT_DIM, OUTPUT_DIM, SRC_PAD_IDX, TGT_PAD_IDX, EMBEDDING_SIZE, NUM_LAYERS, max_length=MAX_LENGTH, dropout=DROUPOUT, device=device).to(device)\n",
    "\n",
    "# 定义优化器和损失函数\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00005, weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=TGT_PAD_IDX)\n",
    "\n",
    "# 使用学习率调度器\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
    "\n",
    "# 训练模型\n",
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for i, (src, tgt) in enumerate(iterator):\n",
    "        # 调整形状\n",
    "        src = src.transpose(0, 1)  # (序列长度, 批次大小) -> (批次大小, 序列长度)\n",
    "        tgt = tgt.transpose(0, 1)  # (序列长度, 批次大小) -> (批次大小, 序列长度)\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, tgt[:, :-1])\n",
    "        output_dim = output.shape[-1]\n",
    "        # output = [batch size, tgt sent len - 1, output dim]\n",
    "        # tgt = [batch size, tgt sent len]\n",
    "        output = output.contiguous().view(-1, output_dim)\n",
    "        tgt = tgt[:,1:].contiguous().view(-1)\n",
    "        # output = [batch size * tgt sent len - 1, output dim]\n",
    "        # tgt = [batch size * tgt sent len - 1]\n",
    "        loss = criterion(output, tgt)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "# 测试模型\n",
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (src, tgt) in enumerate(iterator):\n",
    "            # 调整形状\n",
    "            src = src.transpose(0, 1)  # (序列长度, 批次大小) -> (批次大小, 序列长度)\n",
    "            tgt = tgt.transpose(0, 1)  # (序列长度, 批次大小) -> (批次大小, 序列长度)\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            output = model(src, tgt[:, :-1])\n",
    "            output_dim = output.shape[-1]\n",
    "            # output = [batch size, tgt sent len - 1, output dim]\n",
    "            # tgt = [batch size, tgt sent len]\n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            tgt = tgt[:,1:].contiguous().view(-1)\n",
    "            # output = [batch size * tgt sent len - 1, output dim]\n",
    "            # tgt = [batch size * tgt sent len - 1]\n",
    "            loss = criterion(output, tgt)\n",
    "            epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "# 训练和评估\n",
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "# 创建保存模型的目录\n",
    "model_dir = 'models'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "model_path = os.path.join(model_dir, 'transformer_model.pt')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    train_loss = train(model, train_dataloader, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_dataloader, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = divmod(end_time - start_time, 60)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {int(epoch_mins)}m {int(epoch_secs)}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f}')\n",
    "    # 使用学习率调度器\n",
    "    scheduler.step(valid_loss)\n",
    "    # 保存最好的模型\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "# 加载最好的模型并进行测试\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "test_loss = evaluate(model, test_dataloader, criterion)\n",
    "print(f'Test Loss: {test_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
